@article{jin_sonicasl_2021,
 abstract = {We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8% and 90.6% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians. CCS Concepts: • Human-centered computing → Human computer interaction (HCI); Ubiquitous and mobile computing systems and tools.},
 author = {Jin, Yincheng and Gao, Yang and Zhu, Yanjun and Wang, Wei and Li, Jiyang and Choi, Seokmin and Li, Zhangyu and Chauhan, Jagmohan and Dey, Anind K. and Jin, Zhanpeng},
 copyright = {All rights reserved},
 doi = {10.1145/3463519},
 file = {PDF:/Users/yanggao/Zotero/storage/FWNB5PDH/Jin et al. - 2021 - SonicASL An Acoustic-based Sign Language Gesture Recognizer Using Earphones.pdf:application/pdf},
 issn = {2474-9567},
 journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
 language = {en},
 month = {June},
 number = {2},
 pages = {1--30},
 shorttitle = {SonicASL},
 title = {SonicASL: An Acoustic-based Sign Language Gesture Recognizer Using Earphones},
 url = {https://dl.acm.org/doi/10.1145/3463519},
 urldate = {2025-07-06},
 volume = {5},
 year = {2021}
}
