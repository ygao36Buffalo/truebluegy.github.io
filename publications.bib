
@article{guo_echotouch_2025,
	title = {{EchoTouch}: {Low}-power {Face}-touching {Behavior} {Recognition} {Using} {Active} {Acoustic} {Sensing} on {Glasses}},
	volume = {9},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{EchoTouch}},
	url = {https://dl.acm.org/doi/10.1145/3729481},
	doi = {10.1145/3729481},
	abstract = {Accurately recognizing face-touching behavior at any time and place can help prevent potential health risks and improve personal habits. However, there remains a lack of effective methods that can be applied in real-world scenarios. In this paper, we propose EchoTouch, a low-power, unobtrusive active acoustic sensing system for monitoring face-touching behavior. EchoTouch captures features from both sides of the face by emitting and receiving orthogonal ultrasound signals through two pairs of microphones and speakers mounted along the under frame of glasses. Then, a lightweight and multi-task deep learning framework identifies the touch area and determines whether the behavior is intrusive to prevent such actions. Finally, a two-stage irrelevant action filtering mechanism effectively handles various interferences. We evaluate EchoTouch on 20 individuals using 11 different types of face-touching areas. EchoTouch achieves an average accuracy of 92.9\%, with an 87.2\% accuracy in determining whether the behavior is intrusive. Additionally, in-the-wild evaluations further validate the robustness of EchoTouch. We believe that EchoTouch can serve as an unobtrusive and reliable way to monitor and prevent intrusive face-touching behavior.},
	language = {en},
	number = {2},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Guo, Kaiyi and Wu, Tianyu and Gao, Yang and Zhang, Qian and Wang, Dong},
	month = jun,
	year = {2025},
	pages = {1--33},
	annote = {[PDF] from acm.org},
	file = {Available Version (via Google Scholar):/Users/yanggao/Zotero/storage/MRER2AMQ/Guo et al. - 2025 - EchoTouch Low-power Face-touching Behavior Recognition Using Active Acoustic Sensing on Glasses.pdf:application/pdf},
}

@article{liu_sandtouch_2025,
	title = {{SandTouch}: {Empowering} {Virtual} {Sand} {Art} in {VR} with {AI} {Guidance} and {Emotional} {Relief}},
	copyright = {All rights reserved},
	abstract = {Sand painting is a highly aesthetic and valuable form of art but often constrained by the need for specific equipment and the associated learning curve. To address these challenges, we developed a VR sand painting system, SandTouch, offering an immersive and intuitive sand painting experience that closely mirrors the interaction with physical sand. Leveraging advanced gesture recognition technology, SandTouch allows users to create intricate sand art in a virtual environment, capturing the fine sensations of real sand manipulation along with realistic sound feedback. The integration of AI agent further enhances the experience by intelligently interpreting users’ creative intentions based on real-time interactions, offering contextually relevant artistic suggestions. Comprehensive evaluations have demonstrated a significant increase in user engagement and immersion. Furthermore, the realistic sound feedback enhances emotional relief and deepens the painting experience.},
	language = {en},
	author = {Liu, Long},
	year = {2025},
	file = {PDF:/Users/yanggao/Zotero/storage/N8D7CN3E/Liu - 2025 - SandTouch Empowering Virtual Sand Art in VR with AI Guidance and Emotional Relief.pdf:application/pdf},
}

@article{wei_end--end_2023,
	title = {An {End}-to-{End} {Energy}-{Efficient} {Approach} for {Intake} {Detection} {With} {Low} {Inference} {Time} {Using} {Wrist}-{Worn} {Sensor}},
	volume = {27},
	copyright = {All rights reserved},
	issn = {2168-2208},
	url = {https://ieeexplore.ieee.org/abstract/document/10124956},
	doi = {10.1109/JBHI.2023.3276629},
	abstract = {Automated detection of intake gestures with wearable sensors has been a critical area of research for advancing our understanding and ability to intervene in people's eating behavior. Numerous algorithms have been developed and evaluated in terms of accuracy. However, ensuring the system is not only accurate in making predictions but also efficient in doing so is critical for real-world deployment. Despite the growing research on accurate detection of intake gestures using wearables, many of these algorithms are often energy inefficient, impeding on-device deployment for continuous and real-time monitoring of diet. This article presents a template-based optimized multicenter classifier that enables accurate intake gesture detection while maintaining low-inference time and energy consumption using a wrist-worn accelerometer and gyroscope. We designed an Intake Gesture Counter smartphone application (CountING) and validated the practicality of our algorithm against seven state-of-the-art approaches on three public datasets (In-lab FIC, Clemson, and OREBA). Compared with other methods, we achieved optimal accuracy (81.60\% F1 score) and very low inference time (15.97 msec per 2.20-sec data sample) on the Clemson dataset, and among the top performing algorithms, we achieve comparable accuracy (83.0\% F1 score compared with 85.6\% in the top performing algorithm) but superior inference time (13.8x faster, 33.14 msec per 2.20-sec data sample) on the In-lab FIC dataset and comparable accuracy (83.40\% F1 score compared with 88.10\% in the top-performing algorithm) but superior inference time (33.9x faster, 16.71 msec inference time per 2.20-sec data sample) on the OREBA dataset. On average, our approach achieved a 25-hour battery lifetime (44\% to 52\% improvement over state-of-the-art approaches) when tested on a commercial smartwatch for continuous real-time detection. Our approach demonstrates an effective and efficient method, enabling real-time intake gesture detection using wrist-worn devices in longitudinal studies.},
	number = {8},
	urldate = {2025-07-06},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Wei, Boyang and Zhang, Shibo and Diao, Xingjian and Xu, Qiuyang and Gao, Yang and Alshurafa, Nabil},
	month = aug,
	year = {2023},
	keywords = {accelerometer, activity recognition, Behavioral sciences, biomedical signal processing, energy intake, energy-efficient machine learning algorithm, Feature extraction, gyroscope, Inference algorithms, inference time, Machine learning algorithms, multicenter classifier, Prediction algorithms, Real-time systems, Wearable sensors, Wrist},
	pages = {3878--3888},
	annote = {PDF},
	file = {Full Text PDF:/Users/yanggao/Zotero/storage/SELE6DMQ/Wei et al. - 2023 - An End-to-End Energy-Efficient Approach for Intake Detection With Low Inference Time Using Wrist-Wor.pdf:application/pdf;Snapshot:/Users/yanggao/Zotero/storage/5KJDKXSY/10124956.html:text/html},
}

@inproceedings{adate_detecting_2022,
	title = {Detecting {Screen} {Presence} with {Activity}-{Oriented} {RGB} {Camera} in {Egocentric} {Videos}},
	copyright = {All rights reserved},
	url = {https://ieeexplore.ieee.org/abstract/document/9767433},
	doi = {10.1109/PerComWorkshops53856.2022.9767433},
	abstract = {Screen time is associated with several health risk behaviors including mindless eating, sedentary behavior, and decreased academic performance. Screen time behavior is traditionally assessed with self-report measures, which are known to be burdensome, inaccurate, and imprecise. Recent methods to automatically detect screen time are geared more towards detecting television screens from wearable cameras that record high-resolution video. Activity-oriented wearable cameras (i.e., cameras oriented towards the wearer with a fisheye lens) have recently been designed and shown to reduce privacy concerns, yet pose a greater challenge in capturing screens due to their orientation and fewer pixels on target. Methods that detect screens from low-power, low-resolution wearable camera video are needed given the increased adoption of such devices in longitudinal studies. We propose a method that leverages deep learning algorithms and lower-resolution images from an activity-oriented camera to detect screen presence from multiple types of screens with high variability of pixel on target (e.g., near and far TV, smartphones, laptops, and tablets). We test our system in a real-world study comprising 10 individuals, 80 hours of data, and 1.2 million low-resolution RGB frames. Our results outperform existing state-of-the-art video screen detection methods yielding an F1-score of 81\%. This paper demonstrates the potential for detecting screen-watching behavior in longitudinal studies using activity-oriented cameras, paving the way for a nuanced understanding of screen time’s relationship with health risk behaviors.},
	urldate = {2025-07-06},
	booktitle = {2022 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} and other {Affiliated} {Events} ({PerCom} {Workshops})},
	author = {Adate, Amit and Shahi, Soroush and Alharbi, Rawan and Sen, Sougata and Gao, Yang and Katsaggelos, Aggelos K and Alshurafa, Nabil},
	month = mar,
	year = {2022},
	keywords = {Cameras, Conferences, Deep learning, Egocentric Videos, Fisheye Lens, Object Detection, Pervasive computing, Portable computers, Privacy, TV, Wearable Camera},
	pages = {403--408},
	annote = {PDF},
	file = {Full Text PDF:/Users/yanggao/Zotero/storage/WVR9S75C/Adate et al. - 2022 - Detecting Screen Presence with Activity-Oriented RGB Camera in Egocentric Videos.pdf:application/pdf},
}

@article{gao_heart_2020,
	title = {Heart {Monitor} {Using} {Flexible} {Capacitive} {ECG} {Electrodes}},
	volume = {69},
	copyright = {All rights reserved},
	issn = {1557-9662},
	url = {https://ieeexplore.ieee.org/abstract/document/8882340},
	doi = {10.1109/TIM.2019.2949320},
	abstract = {Noninvasive sensors capable of measuring weak biopotential signals, such as electrocardiogram (ECG) and EEG, and communicating results wirelessly to a host computer are developing rapidly. Some of them utilize capacitively coupled electrodes in an attempt to place the sensor at a distance from the skin. This article demonstrated the fabrication and development of a capacitively coupled ECG electrode prototype using custom high specific capacitance electrodes and custom high-performance electronics. Two ultrathin capacitive electrodes were fabricated on a flexible polyimide substrate (2 × 2 in) protected by a guard ring to reduce noise. The detection and amplification circuitry consisted of operational amplifiers (OpAmps) that filtered and conditioned the ECG signal. R-peaks in the ECG were readily detected and quantified using both simulated signals from ECG databases and real signals from human subjects. Heart rates and heart rate variability calculated from our monitor measurements were comparable with commercial rigid wearable sensors, including a smartwatch and an ECG monitor that uses standard clinical ionic electrodes. The prototype monitor was tested on human subjects during rest and moderate exercise and showed appropriate responses. The challenge of high-gain low-noise amplification was met by the development of highly thinned OpAmps whose operation was shown to be equivalent to commercially available rigidly packaged OpAmps, demonstrating that high-performance Si-electronics can be used to produce high-fidelity signals from weak biopotentials.},
	number = {7},
	urldate = {2025-07-06},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Gao, Yang and Soman, Varun V. and Lombardi, Jack P. and Rajbhandari, Pravakar Prasad and Dhakal, Tara P. and Wilson, Dale G. and Poliks, Mark D. and Ghose, Kanad and Turner, James N. and Jin, Zhanpeng},
	month = jul,
	year = {2020},
	keywords = {Biomedical monitoring, Capacitance, Capacitive electrode, Dielectrics, electrocardiogram (ECG), Electrocardiography, Electrodes, health monitoring, heart rate, Monitoring, Skin},
	pages = {4314--4323},
	annote = {PDF},
	file = {Full Text PDF:/Users/yanggao/Zotero/storage/48IYK5TS/Gao et al. - 2020 - Heart Monitor Using Flexible Capacitive ECG Electrodes.pdf:application/pdf;Snapshot:/Users/yanggao/Zotero/storage/MPBWCSDD/8882340.html:text/html},
}

@article{gao_thermotag_2021,
	title = {{ThermoTag}: {A} {Hidden} {ID} of {3D} {Printers} for {Fingerprinting} and {Watermarking}},
	volume = {16},
	copyright = {All rights reserved},
	issn = {1556-6021},
	shorttitle = {{ThermoTag}},
	url = {https://ieeexplore.ieee.org/abstract/document/9385084},
	doi = {10.1109/TIFS.2021.3065225},
	abstract = {To address the increasing challenges of counterfeit detection and IP protection for 3D printing, we propose that every 3D printer holds unique fingerprinting features characterized by the thermodynamic properties of the extruder hot-end and can be used as a new way of 3D watermarking. We prove that these physical fingerprints resulting from manufacturing imperfections and system variations exhibit distinct heating responses, namely “ThermoTag,” which can be represented as the distinguishable thermodynamic processes and, ultimately, the temperature readings during the preheating process. Experimental results show that, by only changing the hot-ends of the same model on the same 3D printer, we can achieve about 92\% identification accuracy amongst 45 hot-ends. The permanence and robustness of ThermoTag for the same hot-end were examined, throughout a period of one month with hundreds of trials under different environmental temperature settings. Leveraging the hidden ThermoTag, an example of watermarking scheme in 3D printing is presented and evaluated.},
	urldate = {2025-07-06},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Gao, Yang and Wang, Wei and Jin, Yincheng and Zhou, Chi and Xu, Wenyao and Jin, Zhanpeng},
	year = {2021},
	keywords = {3D printer, Computational modeling, fingerprinting, Frequency division multiplexing, hot-end, Printers, Solid modeling, thermal model, Three-dimensional displays, Three-dimensional printing, watermarking, Watermarking},
	pages = {2805--2820},
	annote = {PDF},
	file = {Full Text PDF:/Users/yanggao/Zotero/storage/NC9Z6BAV/Gao et al. - 2021 - ThermoTag A Hidden ID of 3D Printers for Fingerprinting and Watermarking.pdf:application/pdf},
}

@inproceedings{xiao_wrist_2025,
	address = {Yokohama Japan},
	title = {From {Wrist} to {Finger}: {Hand} {Pose} {Tracking} {Using} {Ring}-{Watch} {Wearables}},
	copyright = {All rights reserved},
	isbn = {979-8-4007-1395-8},
	shorttitle = {From {Wrist} to {Finger}},
	url = {https://dl.acm.org/doi/10.1145/3706599.3720220},
	doi = {10.1145/3706599.3720220},
	abstract = {Hand pose tracking is essential for advancing applications in humancomputer interaction. Current approaches, such as vision-based systems and wearable devices, face limitations in portability, usability, and practicality. This paper proposes a novel multimodal hand pose tracking framework that integrates data from an IMU-equipped ring and EMG sensors embedded in a wrist-worn device. By leveraging the complementary strengths of motion dynamics and muscle activity, our deep learning-based sensor fusion approach achieves precise 3D hand pose reconstruction. We fused multichannel data using a transformer-based model incorporating time encoding and cross-modal attention mechanisms. We also designed weighted loss function designed to optimize spatial, kinematic, and anatomical accuracy. Experimental validation using a custom dataset of 19 gestures performed by 10 participants demonstrates robust performance, with an average MPJPE of 0.750 cm and joint angle differences of 6.815° for cross-user evaluation.},
	language = {en},
	urldate = {2025-07-06},
	booktitle = {Proceedings of the {Extended} {Abstracts} of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Xiao, Yingjing and Huang, ZhiChao and Gao, Yang},
	month = apr,
	year = {2025},
	pages = {1--7},
	file = {PDF:/Users/yanggao/Zotero/storage/LG3QXHZ2/Xiao et al. - 2025 - From Wrist to Finger Hand Pose Tracking Using Ring-Watch Wearables.pdf:application/pdf},
}

@article{li_signring_2023,
	title = {{SignRing}: {Continuous} {American} {Sign} {Language} {Recognition} {Using} {IMU} {Rings} and {Virtual} {IMU} {Data}},
	volume = {7},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{SignRing}},
	url = {https://dl.acm.org/doi/10.1145/3610881},
	doi = {10.1145/3610881},
	abstract = {Sign language is a natural language widely used by Deaf and hard of hearing (DHH) individuals. Advanced wearables are developed to recognize sign language automatically. However, they are limited by the lack of labeled data, which leads to a small vocabulary and unsatisfactory performance even though laborious efforts are put into data collection. Here we propose SignRing, an IMU-based system that breaks through the traditional data augmentation method, makes use of online videos to generate the virtual IMU (v-IMU) data, and pushes the boundary of wearable-based systems by reaching the vocabulary size of 934 with sentences up to 16 glosses. The v-IMU data is generated by reconstructing 3D hand movements from two-view videos and calculating 3-axis acceleration data, by which we are able to achieve a word error rate (WER) of 6.3\% with a mix of half v-IMU and half IMU training data (2339 samples for each), and a WER of 14.7\% with 100\% v-IMU training data (6048 samples), compared with the baseline performance of the 8.3\% WER (trained with 2339 samples of IMU data). We have conducted comparisons between v-IMU and IMU data to demonstrate the reliability and generalizability of the v-IMU data. This interdisciplinary work covers various areas such as wearable sensor development, computer vision techniques, deep learning, and linguistics, which can provide valuable insights to researchers with similar research objectives.},
	language = {en},
	number = {3},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Li, Jiyang and Huang, Lin and Shah, Siddharth and Jones, Sean J. and Jin, Yincheng and Wang, Dingran and Russell, Adam and Choi, Seokmin and Gao, Yang and Yuan, Junsong and Jin, Zhanpeng},
	month = sep,
	year = {2023},
	pages = {1--29},
	file = {PDF:/Users/yanggao/Zotero/storage/GRL9UKLI/Li et al. - 2023 - SignRing Continuous American Sign Language Recognition Using IMU Rings and Virtual IMU Data.pdf:application/pdf},
}

@article{gao_voice_2021,
	title = {Voice {In} {Ear}: {Spoofing}-{Resistant} and {Passphrase}-{Independent} {Body} {Sound} {Authentication}},
	volume = {5},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {Voice {In} {Ear}},
	url = {https://dl.acm.org/doi/10.1145/3448113},
	doi = {10.1145/3448113},
	abstract = {With the rapid growth of wearable computing and increasing demand for mobile authentication scenarios, voiceprint-based authentication has become one of the prevalent technologies and has already presented tremendous potentials to the public. However, it is vulnerable to voice spoofing attacks (e.g., replay attacks and synthetic voice attacks). To address this threat, we propose a new biometric authentication approach, named EarPrint, which aims to extend voiceprint and build a hidden and secure user authentication scheme on earphones. EarPrint builds on the speaking-induced body sound transmission from the throat to the ear canal, i.e., different users will have different body sound conduction patterns on both sides of ears. As the first exploratory study, extensive experiments on 23 subjects show the EarPrint is robust against ambient noises and body motions. EarPrint achieves an Equal Error Rate (EER) of 3.64\% with 75 seconds enrollment data. We also evaluate the resilience of EarPrint against replay attacks. A major contribution of EarPrint is that it leverages two-level uniqueness, including the body sound conduction from the throat to the ear canal and the body asymmetry between the left and the right ears, taking advantage of earphones' paring form-factor. Compared with other mobile and wearable biometric modalities, EarPrint is a low-cost, accurate, and secure authentication solution for earphone users.},
	language = {en},
	number = {1},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Gao, Yang and Jin, Yincheng and Chauhan, Jagmohan and Choi, Seokmin and Li, Jiyang and Jin, Zhanpeng},
	month = mar,
	year = {2021},
	pages = {1--25},
	file = {PDF:/Users/yanggao/Zotero/storage/WYPF5RUM/Gao et al. - 2021 - Voice In Ear Spoofing-Resistant and Passphrase-Independent Body Sound Authentication.pdf:application/pdf},
}

@article{jin_smartasl_2023,
	title = {{SmartASL}: "{Point}-of-{Care}" {Comprehensive} {ASL} {Interpreter} {Using} {Wearables}},
	volume = {7},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{SmartASL}},
	url = {https://dl.acm.org/doi/10.1145/3596255},
	doi = {10.1145/3596255},
	abstract = {Sign language builds up an important bridge between the d/Deaf and hard-of-hearing (DHH) and hearing people. Regrettably, most hearing people face challenges in comprehending sign language, necessitating sign language translation. However, state-of-the-art wearable-based techniques mainly concentrate on recognizing manual markers (e.g., hand gestures), while frequently overlooking non-manual markers, such as negative head shaking, question markers, and mouthing. This oversight results in the loss of substantial grammatical and semantic information in sign language. To address this limitation, we introduce SmartASL, a novel proof-of-concept system that can 1) recognize both manual and non-manual markers simultaneously using a combination of earbuds and a wrist-worn IMU, and 2) translate the recognized American Sign Language (ASL) glosses into spoken language. Our experiments demonstrate the SmartASL system's significant potential to accurately recognize the manual and non-manual markers in ASL, effectively bridging the communication gaps between ASL signers and hearing people using commercially available devices.},
	language = {en},
	number = {2},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Jin, Yincheng and Zhang, Shibo and Gao, Yang and Xu, Xuhai and Choi, Seokmin and Li, Zhengxiong and Adler, Henry J. and Jin, Zhanpeng},
	month = jun,
	year = {2023},
	pages = {1--21},
	file = {PDF:/Users/yanggao/Zotero/storage/EEK4MVRY/Jin et al. - 2023 - SmartASL Point-of-Care Comprehensive ASL Interpreter Using Wearables.pdf:application/pdf},
}

@article{gao_sonicface_2021,
	title = {{SonicFace}: {Tracking} {Facial} {Expressions} {Using} a {Commodity} {Microphone} {Array}},
	volume = {5},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{SonicFace}},
	url = {https://dl.acm.org/doi/10.1145/3494988},
	doi = {10.1145/3494988},
	abstract = {Accurate recognition of facial expressions and emotional gestures is promising to understand the audience's feedback and engagement on the entertainment content. Existing methods are primarily based on various cameras or wearable sensors, which either raise privacy concerns or demand extra devices. To this aim, we propose a novel ubiquitous sensing system based on the commodity microphone array --- SonicFace, which provides an accessible, unobtrusive, contact-free, and privacy-preserving solution to monitor the user's emotional expressions continuously without playing hearable sound. SonicFace utilizes a pair of speaker and microphone array to recognize various fine-grained facial expressions and emotional hand gestures by emitted ultrasound and received echoes. Based on a set of experimental evaluations, the accuracy of recognizing 6 common facial expressions and 4 emotional gestures can reach around 80\%. Besides, the extensive system evaluations with distinct configurations and an extended real-life case study have demonstrated the robustness and generalizability of the proposed SonicFace system.},
	language = {en},
	number = {4},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Gao, Yang and Jin, Yincheng and Choi, Seokmin and Li, Jiyang and Pan, Junjie and Shu, Lin and Zhou, Chi and Jin, Zhanpeng},
	month = dec,
	year = {2021},
	pages = {1--33},
	file = {PDF:/Users/yanggao/Zotero/storage/HP9SSHQ7/Gao et al. - 2021 - SonicFace Tracking Facial Expressions Using a Commodity Microphone Array.pdf:application/pdf},
}

@article{jin_sonicasl_2021,
	title = {{SonicASL}: {An} {Acoustic}-based {Sign} {Language} {Gesture} {Recognizer} {Using} {Earphones}},
	volume = {5},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{SonicASL}},
	url = {https://dl.acm.org/doi/10.1145/3463519},
	doi = {10.1145/3463519},
	abstract = {We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8\% and 90.6\% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians. CCS Concepts: • Human-centered computing → Human computer interaction (HCI); Ubiquitous and mobile computing systems and tools.},
	language = {en},
	number = {2},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Jin, Yincheng and Gao, Yang and Zhu, Yanjun and Wang, Wei and Li, Jiyang and Choi, Seokmin and Li, Zhangyu and Chauhan, Jagmohan and Dey, Anind K. and Jin, Zhanpeng},
	month = jun,
	year = {2021},
	pages = {1--30},
	file = {PDF:/Users/yanggao/Zotero/storage/FWNB5PDH/Jin et al. - 2021 - SonicASL An Acoustic-based Sign Language Gesture Recognizer Using Earphones.pdf:application/pdf},
}

@article{gao_pressinpose_2024,
	title = {{PressInPose}: {Integrating} {Pressure} and {Inertial} {Sensors} for {Full}-{Body} {Pose} {Estimation} in {Activities}},
	volume = {8},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{PressInPose}},
	url = {https://dl.acm.org/doi/10.1145/3699773},
	doi = {10.1145/3699773},
	abstract = {The accurate assessment of human body posture through wearable technology has significant implications for sports science, clinical diagnostics, rehabilitation, and VR interaction. Traditional methods often require complex setups or are limited by the environment’s constraints. In response to these challenges, this paper presents an innovative approach to human posture estimation under complex motion scenarios through the development of an advanced shoe insole embedded with pressure sensors and an Inertial Measurement Unit (IMU). Coupled with a single wrist-mounted IMU, our system facilitates a comprehensive analysis of human biomechanics by integrating physical kinematics modeling based on pressure data with a multi-region human posture estimation network. To enhance the robustness of our system model, we employed large language models to generate virtual human motion sequences. These sequences were utilized to create synthetic IMU data for data augmentation purposes, addressing the challenge of limited real-world data availability and variability. Our approach uniquely combines physical modeling with data-driven techniques to improve the accuracy and reliability of posture estimation. Experimental results demonstrate that our integrated system significantly advances wearable technology for motion analysis. The Mean Per Joint Position Error (MPJPE) was reduced to 7.75 cm, highlighting the effectiveness of our multi-modal modeling and virtual data augmentation in refining posture estimation. CCS Concepts: • Human-centered computing → Human computer interaction (HCI); Ubiquitous and mobile computing systems and tools.},
	language = {en},
	number = {4},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Gao, Yang and Zhang, Wenbo and Ren, Junbin and Zheng, Ruihao and Jin, Yingcheng and Wu, Di and Shu, Lin and Xu, Xiangmin and Jin, Zhanpeng},
	month = nov,
	year = {2024},
	pages = {1--28},
	file = {PDF:/Users/yanggao/Zotero/storage/ZZ8ZCJV4/Gao et al. - 2024 - PressInPose Integrating Pressure and Inertial Sensors for Full-Body Pose Estimation in Activities.pdf:application/pdf},
}

@article{choi_ppgface_2022,
	title = {{PPGface}: {Like} {What} {You} {Are} {Watching}? {Earphones} {Can} "{Feel}" {Your} {Facial} {Expressions}},
	volume = {6},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{PPGface}},
	url = {https://dl.acm.org/doi/10.1145/3534597},
	doi = {10.1145/3534597},
	abstract = {Recognition of facial expressions has been widely explored to represent people's emotional states. Existing facial expression recognition systems primarily rely on external cameras which make it less accessible and efficient in many real-life scenarios to monitor an individual's facial expression in a convenient and unobtrusive manner. To this end, we propose PPGface, a ubiquitous, easy-to-use, user-friendly facial expression recognition platform that leverages earable devices with built-in PPG sensor. PPGface understands the facial expressions through the dynamic PPG patterns resulting from facial muscle movements. With the aid of the accelerometer sensor, PPGface can detect and recognize the user's seven universal facial expressions and relevant body posture unobtrusively. We conducted an user study (N=20) using multimodal ResNet to evaluate the performance of PPGface, and showed that PPGface can detect different facial expressions with 93.5 accuracy and 0.93 fl-score. In addition, to explore the robustness and usability of our proposed platform, we conducted several comprehensive experiments under real-world settings. Overall results of this work validate a great potential to be employed in future commodity earable devices.},
	language = {en},
	number = {2},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Choi, Seokmin and Gao, Yang and Jin, Yincheng and Kim, Se Jun and Li, Jiyang and Xu, Wenyao and Jin, Zhanpeng},
	month = jul,
	year = {2022},
	pages = {1--32},
	file = {PDF:/Users/yanggao/Zotero/storage/WGEG44S6/Choi et al. - 2022 - PPGface Like What You Are Watching Earphones Can Feel Your Facial Expressions.pdf:application/pdf},
}

@article{jin_earcommand_2022,
	title = {{EarCommand}: "{Hearing}" {Your} {Silent} {Speech} {Commands} {In} {Ear}},
	volume = {6},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{EarCommand}},
	url = {https://dl.acm.org/doi/10.1145/3534613},
	doi = {10.1145/3534613},
	abstract = {Intelligent speech interfaces have been developing vastly to support the growing demands for convenient control and interaction with wearable/earable and portable devices. To avoid privacy leakage during speech interactions and strengthen the resistance to ambient noise, silent speech interfaces have been widely explored to enable people's interaction with mobile/wearable devices without audible sounds. However, most existing silent speech solutions require either restricted background illuminations or hand involvement to hold device or perform gestures. In this study, we propose a novel earphone-based, hand-free silent speech interaction approach, named EarCommand. Our technique discovers the relationship between the deformation of the ear canal and the movements of the articulator and takes advantage of this link to recognize different silent speech commands. Our system can achieve a WER (word error rate) of 10.02\% for word-level recognition and 12.33\% for sentence-level recognition, when tested in human subjects with 32 word-level commands and 25 sentence-level commands, which indicates the effectiveness of inferring silent speech commands. Moreover, EarCommand shows high reliability and robustness in a variety of configuration settings and environmental conditions. It is anticipated that EarCommand can serve as an efficient, intelligent speech interface for hand-free operation, which could significantly improve the quality and convenience of interactions.},
	language = {en},
	number = {2},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Jin, Yincheng and Gao, Yang and Xu, Xuhai and Choi, Seokmin and Li, Jiyang and Liu, Feng and Li, Zhengxiong and Jin, Zhanpeng},
	month = jul,
	year = {2022},
	pages = {1--28},
	file = {PDF:/Users/yanggao/Zotero/storage/QB92TQ8W/Jin et al. - 2022 - EarCommand Hearing Your Silent Speech Commands In Ear.pdf:application/pdf},
}

@article{gao_watching_2018,
	title = {Watching and {Safeguarding} {Your} {3D} {Printer}: {Online} {Process} {Monitoring} {Against} {Cyber}-{Physical} {Attacks}},
	volume = {2},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {Watching and {Safeguarding} {Your} {3D} {Printer}},
	url = {https://dl.acm.org/doi/10.1145/3264918},
	doi = {10.1145/3264918},
	abstract = {The increasing adoption of 3D printing in many safety and mission critical applications exposes 3D printers to a variety of cyber attacks that may result in catastrophic consequences if the printing process is compromised. For example, the mechanical properties (e.g., physical strength, thermal resistance, dimensional stability) of 3D printed objects could be significantly affected and degraded if a simple printing setting is maliciously changed. To address this challenge, this study proposes a model-free real-time online process monitoring approach that is capable of detecting and defending against the cyber-physical attacks on the firmwares of 3D printers. Specifically, we explore the potential attacks and consequences of four key printing attributes (including infill path, printing speed, layer thickness, and fan speed) and then formulate the attack models. Based on the intrinsic relation between the printing attributes and the physical observations, our defense model is established by systematically analyzing the multi-faceted, real-time measurement collected from the accelerometer, magnetometer and camera. The Kalman filter and Canny filter are used to map and estimate three aforementioned critical toolpath information that might affect the printing quality. Mel-frequency Cepstrum Coefficients are used to extract features for fan speed estimation. Experimental results show that, for a complex 3D printed design, our method can achieve 4\% Hausdorff distance compared with the model dimension for infill path estimate, 6.07\% Mean Absolute Percentage Error (MAPE) for speed estimate, 9.57\% MAPE for layer thickness estimate, and 96.8\% accuracy for fan speed identification. Our study demonstrates that, this new approach can effectively defend against the cyber-physical attacks on 3D printers and 3D printing process.},
	language = {en},
	number = {3},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Gao, Yang and Li, Borui and Wang, Wei and Xu, Wenyao and Zhou, Chi and Jin, Zhanpeng},
	month = sep,
	year = {2018},
	pages = {1--27},
	file = {PDF:/Users/yanggao/Zotero/storage/X2EHDGZ2/Gao et al. - 2018 - Watching and Safeguarding Your 3D Printer Online Process Monitoring Against Cyber-Physical Attacks.pdf:application/pdf},
}

@article{gao_earecho_2019,
	title = {{EarEcho}: {Using} {Ear} {Canal} {Echo} for {Wearable} {Authentication}},
	volume = {3},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{EarEcho}},
	url = {https://dl.acm.org/doi/10.1145/3351239},
	doi = {10.1145/3351239},
	abstract = {Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19\% and precision of 95.16\% for one-time authentication, while a recall of 97.55\% and precision of 97.57\% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.},
	language = {en},
	number = {3},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Gao, Yang and Wang, Wei and Phoha, Vir V. and Sun, Wei and Jin, Zhanpeng},
	month = sep,
	year = {2019},
	pages = {1--24},
	file = {PDF:/Users/yanggao/Zotero/storage/VFYL3DE8/Gao et al. - 2019 - EarEcho Using Ear Canal Echo for Wearable Authentication.pdf:application/pdf},
}

@article{gao_echowhisper_2020,
	title = {{EchoWhisper}: {Exploring} an {Acoustic}-based {Silent} {Speech} {Interface} for {Smartphone} {Users}},
	volume = {4},
	copyright = {All rights reserved},
	issn = {2474-9567},
	shorttitle = {{EchoWhisper}},
	url = {https://dl.acm.org/doi/10.1145/3411830},
	doi = {10.1145/3411830},
	abstract = {With the rapid growth of artificial intelligence and mobile computing, intelligent speech interface has recently become one of the prevalent trends and has already presented huge potentials to the public. To address the privacy leakage issue during the speech interaction or accommodate some special demands, silent speech interfaces have been proposed to enable people’s communication without vocalizing their sound (e.g., lip reading, tongue tracking). However, most existing silent speech mechanisms require either background illuminations or additional wearable devices. In this study, we propose the EchoWhisper as a novel user-friendly, smartphone-based silent speech interface. The proposed technique takes advantage of the micro-Doppler effect of the acoustic wave resulting from mouth and tongue movements and assesses the acoustic features of beamformed reflected echoes captured by the dual microphones in the smartphone. Using human subjects who perform a daily conversation task with over 45 different words, our system can achieve a WER (word error rate) of 8.33\%, which shows the effectiveness of inferring silent speech content. Moreover, EchoWhisper has also demonstrated its reliability and robustness to a variety of configuration settings and environmental factors, such as smartphone orientations and distances, ambient noises, body motions, and so on. CCS Concepts: • Human-centered computing → Human computer interaction (HCI); Mobile devices; • Computer systems organization → Sensors and actuators.},
	language = {en},
	number = {3},
	urldate = {2025-07-06},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Gao, Yang and Jin, Yincheng and Li, Jiyang and Choi, Seokmin and Jin, Zhanpeng},
	month = sep,
	year = {2020},
	pages = {1--27},
	file = {PDF:/Users/yanggao/Zotero/storage/6SD2FZ9S/Gao et al. - 2020 - EchoWhisper Exploring an Acoustic-based Silent Speech Interface for Smartphone Users.pdf:application/pdf},
}
